# -*- coding: utf-8 -*-
"""CSE437 project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kMAvIjUtQvnycDoIL_W1OR_Sx_-SnTeY

## Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""## Importing dataset"""

df = pd.read_csv('/content/Student_depression_dataset.csv')

"""## Dataset Description"""

# features
print(f'Total fetures: {df.shape[1] - 1}')

# Regression or classification
print('This is a classification problem. Because we only need to evaluate whether a student has depression or not.')

# Data points
print(f'Total data points: {df.shape[0]}')

# Type of feature
print(set(df.dtypes))

"""Imbalanced dataset"""

df['Depression'].value_counts()

"""Bar chart"""

depression_counts = df['Depression'].value_counts()
plt.bar(depression_counts.index, depression_counts.values)
plt.xlabel('Depression status')
plt.ylabel('Count')
plt.title('Depression status count')
plt.xticks(ticks=[0, 1], labels=['Not Depressed', 'Depressed'])
plt.show()

"""## Data pre-processing"""

df.head()

df.shape

"""checking the null values"""

df.isnull().sum()

"""### Dropping columns

We will drop the ID, and Profession column.

The ID does not serve any useful purpose for our model training as it's a unique value for each student.

The profession does not serve any useful purpose for our model because since it's a student dataset, the profession of all the students is 'student'. Although a few students have different profession, that ultimately creates outlier in the dataset.
"""

# drop the columns id, Profession
df = df.drop(['id', 'Profession'], axis = 1)

"""### Dropping null values

We can see that there are very few null values in CGPA, and Financial Stress column
"""

# dropping rows of CGPA that have null values
df = df.dropna(axis = 0, subset = ['CGPA'])

# # dropping rows of Job Satisfaction that have null values
# df = df.dropna(axis = 0, subset = ['Job Satisfaction'])

# dropping rows of Financial Stress that have null values
df = df.dropna(axis = 0, subset = ['Financial Stress'])

# checking the shape after dropping null values
df.shape

"""### Imputing values

In the 'Job Staisfaction' column, there are a lot of null values
"""

df_job_none = df[df['Job Satisfaction'].isnull()]
df_job_none

df_job_none['Work Pressure'].value_counts()

"""From the above code, we can see that Job satisfaction is null only when the Work pressure is 0 indicating that in the case of Job satisfaction being a null value, a student doesn't have a job

Imputing the values with 0
"""

df.fillna(0, inplace=True)
df.head()

"""### Encoding

#### Binary encoding
"""

binary_enc = LabelEncoder()
columns_to_encode = ['Gender', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']
for column in columns_to_encode:
    df[column] = binary_enc.fit_transform(df[column])

df.head()

"""#### One hot encoding"""

# One hot encoding
categories = ['City', 'Degree']
# for category in categories:
#     df[category] = df[category].astype('category')
df = pd.get_dummies(df, columns = categories, dtype = int)

df.shape

df.head()

"""### Mapping values for ranking columns"""

# Sleep Duration
df['Sleep Duration'].value_counts()

"""Removing 'Others' due to very low counts."""

df = df[df['Sleep Duration'] != 'Others']

df['Sleep Duration'] = df['Sleep Duration'].map({
    '7-8 hours' : 4,
    '5-6 hours' : 3,
    'More than 8 hours' : 2,
    'Less than 5 hours' : 1
})

# Dietary Habits
df['Dietary Habits'].value_counts()

"""Removing 'Others' due to very low counts."""

df = df[df['Dietary Habits'] != 'Others']

df['Dietary Habits'] = df['Dietary Habits'].map({
    'Healthy' : 3,
    'Moderate' : 2,
    'Unhealthy' : 1
})

df.head()

df.shape

"""### Correlation"""

print(set(df.dtypes))

df_corr = df.corr()
sns.heatmap(df_corr, cmap = 'YlGnBu')

"""### Split into train and test dataset"""

x = df.drop('Depression', axis = 1)
y = df['Depression']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify = y, random_state=42)

"""### feature scaling"""

scaler = MinMaxScaler()

X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.fit_transform(x_test)

"""## Applying ML

### Logistic Regression
"""

logistic_model = LogisticRegression()
logistic_model.fit(X_train_scaled, y_train)

y_pred = logistic_model.predict(X_test_scaled)

"""Accuracy"""

# accuracy
logistic_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", logistic_accuracy)

"""Confusion matrix"""

# confusion matrix
logistic_cm = confusion_matrix(y_test, y_pred)
print('confusion matrix:')
print(logistic_cm)

"""Visualize Confusion matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(logistic_cm, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Logistic Regression Confusion Matrix without PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification report"""

logistic_class_report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(logistic_class_report)

report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Logistic Regression Classification Report Heatmap without PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###XGBoost"""

xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train_scaled, y_train)

y_pred = xgb_model.predict(X_test_scaled)

"""Accuracy"""

xgb_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", xgb_accuracy)

"""Confusion Matrix"""

xgb_cm = confusion_matrix(y_test, y_pred)
print('confusion matrix:')
print(xgb_cm)

"""Visualize Confusion matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('XGBoost Confusion Matrix without PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

xgb_class_report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(xgb_class_report)

report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('XGBoost Classification Report Heatmap without PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###Random Forest"""

rf_model = RandomForestClassifier()
rf_model.fit(X_train_scaled, y_train)

y_pred = rf_model.predict(X_test_scaled)

"""Accuracy"""

rf_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", rf_accuracy)

"""Confusion Matrix"""

rf_cm = confusion_matrix(y_test, y_pred)
print('confusion matrix:')
print(rf_cm)

"""Visualizing Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Random Forest Confusion Matrix without PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

rf_class_report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(rf_class_report)

report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Random Forest Classification Report Heatmap without PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###SVM (Support Vector Machine)"""

kernels = ['linear', 'poly', 'rbf', 'sigmoid']
svm_accuracy = 0
svm_kernel = None
best_y_pred = None

for kernel in kernels:
    svm_model = SVC(kernel = kernel)
    svm_model.fit(X_train_scaled, y_train)

    y_pred = svm_model.predict(X_test_scaled)

    current_accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy for {kernel}: {current_accuracy}")
    if current_accuracy > svm_accuracy:
        svm_accuracy = current_accuracy
        svm_kernel = kernel
        best_y_pred = y_pred

"""Accuracy"""

print("kernel:", svm_kernel)
print("Accuracy:", svm_accuracy)

"""Confusion Matrix"""

svm_cm = confusion_matrix(y_test, best_y_pred)
print('confusion matrix:')
print(svm_cm)

"""VIsualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('SVM Confusion Matrix without PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

svm_class_report = classification_report(y_test, best_y_pred)
print("\nClassification Report:")
print(svm_class_report)

report = classification_report(y_test, best_y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('SVM Classification Report Heatmap without PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""
### KNN"""

knn = KNeighborsClassifier()
knn.fit(X_train_scaled, y_train)

y_pred = knn.predict(X_test_scaled)

"""Accuracy"""

# accuracy
knn_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", knn_accuracy)

"""confusion matrix

"""

# confusion matrix
knn_cm = confusion_matrix(y_test, y_pred)
print('confusion matrix:')
print(knn_cm)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(knn_cm, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('KNN Confusion Matrix without PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification report"""

knn_class_report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(knn_class_report)

report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('KNN Classification Report Heatmap without PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""### Decision Tree"""

tree_model = DecisionTreeClassifier(random_state=42)
tree_model.fit(X_train_scaled, y_train)

y_pred = tree_model.predict(X_test_scaled)

"""Accuracy"""

# accuarcy
tree_accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', tree_accuracy)

"""Confusion matrix"""

# confusion matrix
tree_cm = confusion_matrix(y_test, y_pred)
print('confusion matrix:')
print(tree_cm)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(tree_cm, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Decision Tree Confusion Matrix without PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification report"""

tree_class_report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(tree_class_report)

report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Decision Tree Classification Report Heatmap without PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""### Naive Bayes"""

gnb_model = GaussianNB()
gnb_model.fit(x_train, y_train)

y_pred = gnb_model.predict(x_test)

"""Accuracy"""

# accuarcy
gnb_accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', gnb_accuracy)

"""Confusion matrix"""

# confusion matrix
gnb_cm = confusion_matrix(y_test, y_pred)
print('confusion matrix:')
print(gnb_cm)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(gnb_cm, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Naive Bayes Confusion Matrix without PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification report"""

gnb_class_report = classification_report(y_test, y_pred)
print("\nClassification Report:")
print(gnb_class_report)

report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Naive Bayes Classification Report Heatmap without PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""### Bar plot"""

accuracy_data = {
    'Model': ['Logistic Regression', 'XGBoost', 'Random Forest', 'SVM', 'KNN', 'Decision Tree', 'Naive Bayes'],
    'Accuracy': [logistic_accuracy, xgb_accuracy, rf_accuracy, svm_accuracy, knn_accuracy, tree_accuracy, gnb_accuracy]
}

accuracy_df = pd.DataFrame(accuracy_data)

# Plotting the bar plot
plt.figure(figsize=(8, 6))
bars = plt.bar(accuracy_df['Model'], accuracy_df['Accuracy'], color='skyblue')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison without PCA')
plt.ylim(0, 1)  # Set y-axis limit to 0-1 for accuracy

# Rotate the x-axis labels to avoid overlap
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Adding accuracy labels inside the bars
for bar in bars:
    yval = bar.get_height()  # Get the height of the bar (accuracy value)
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.02, f'{yval:.3f}', ha='center', va='bottom', fontsize=10)

plt.show()

"""##Working with PCA

###Apply PCA to reduce dimensionality
"""

pca = PCA(n_components = 0.75)

# Using the already scaled dataset
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.title('Explained Variance by Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.tight_layout()
plt.show()

"""##Applying ML into PCA-reduced dataset

###Logistic Regression
"""

logistic_model_pca = LogisticRegression()
logistic_model_pca.fit(X_train_pca, y_train)

y_pred_pca = logistic_model_pca.predict(X_test_pca)

"""Accuracy"""

logistic_accuracy_pca = accuracy_score(y_test, y_pred_pca)
print("Accuracy:", logistic_accuracy_pca)

"""Confusion Matrix"""

logistic_cm_pca = confusion_matrix(y_test, y_pred_pca)
print('confusion matrix:')
print(logistic_cm_pca)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(logistic_cm_pca, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Logistic Regression Confusion Matrix with PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

logistic_class_report_pca = classification_report(y_test, y_pred_pca)
print("\nClassification Report:")
print(logistic_class_report_pca)

report = classification_report(y_test, y_pred_pca, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Logistic Regression Classification Report Heatmap with PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###XGBoost"""

xgb_model_pca = xgb.XGBClassifier()
xgb_model_pca.fit(X_train_pca, y_train)

y_pred_pca = xgb_model_pca.predict(X_test_pca)

"""Accuracy"""

xgb_accuracy_pca = accuracy_score(y_test, y_pred_pca)
print("Accuracy:", xgb_accuracy_pca)

"""Confusion Matrix"""

xgb_cm_pca = confusion_matrix(y_test, y_pred_pca)
print('confusion matrix:')
print(xgb_cm_pca)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(xgb_cm_pca, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('XGBoost Confusion Matrix with PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

xgb_class_report_pca = classification_report(y_test, y_pred_pca)
print("\nClassification Report:")
print(xgb_class_report_pca)

report = classification_report(y_test, y_pred_pca, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('XGBoost Classification Report Heatmap with PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###Random Forest"""

rf_model_pca = RandomForestClassifier()
rf_model_pca.fit(X_train_pca, y_train)

y_pred_pca = rf_model_pca.predict(X_test_pca)

"""Accuracy"""

rf_accuracy_pca = accuracy_score(y_test, y_pred_pca)
print("Accuracy:", rf_accuracy_pca)

"""Confusion Matrix"""

rf_cm_pca = confusion_matrix(y_test, y_pred_pca)
print('confusion matrix:')
print(rf_cm_pca)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(rf_cm_pca, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Random Forest Confusion Matrix with PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

rf_class_report_pca = classification_report(y_test, y_pred_pca)
print("\nClassification Report:")
print(rf_class_report_pca)

report = classification_report(y_test, y_pred_pca, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Random Forest Classification Report Heatmap with PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###SVM (Support Vector Machine)"""

kernels = ['linear', 'poly', 'rbf', 'sigmoid']
svm_accuracy_pca = 0
svm_kernel_pca = None
best_y_pca_pred = None

for kernel in kernels:
    svm_model = SVC(kernel = kernel)
    svm_model.fit(X_train_pca, y_train)

    y_pred_pca = svm_model.predict(X_test_pca)

    current_accuracy = accuracy_score(y_test, y_pred_pca)
    print(f"Accuracy for {kernel}: {current_accuracy}")
    if current_accuracy > svm_accuracy_pca:
        svm_accuracy_pca = current_accuracy
        svm_kernel_pca = kernel
        best_y_pca_pred = y_pred

"""Accuracy"""

print("kernel:", svm_kernel_pca)
print("Accuracy:", svm_accuracy_pca)

"""Confusion Matrix"""

svm_cm_pca = confusion_matrix(y_test, best_y_pca_pred)
print('confusion matrix:')
print(svm_cm_pca)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(svm_cm_pca, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('SVM Confusion Matrix with PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

svm_pca_class_report = classification_report(y_test, best_y_pca_pred)
print("\nClassification Report:")
print(svm_pca_class_report)

report = classification_report(y_test, best_y_pca_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('SVM Classification Report Heatmap with PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###KNN"""

knn_model_pca = KNeighborsClassifier()
knn_model_pca.fit(X_train_pca, y_train)

y_pred_pca = knn_model_pca.predict(X_test_pca)

"""Accuracy"""

knn_accuracy_pca = accuracy_score(y_test, y_pred_pca)
print("Accuracy:", knn_accuracy_pca)

"""Confusion Matrix"""

knn_cm_pca = confusion_matrix(y_test, y_pred_pca)
print('confusion matrix:')
print(knn_cm_pca)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(knn_cm_pca, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('KNN Confusion Matrix with PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

knn_class_report_pca = classification_report(y_test, y_pred_pca)
print("\nClassification Report:")
print(knn_class_report_pca)

report = classification_report(y_test, y_pred_pca, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('KNN Classification Report Heatmap with PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###Decision Tree"""

tree_model_pca = DecisionTreeClassifier()
tree_model_pca.fit(X_train_pca, y_train)

y_pred_pca = tree_model_pca.predict(X_test_pca)

"""Accuracy"""

tree_accuracy_pca = accuracy_score(y_test, y_pred_pca)
print("Accuracy:", tree_accuracy_pca)

"""Confusion Matrix"""

tree_cm_pca = confusion_matrix(y_test, y_pred_pca)
print('confusion matrix:')
print(tree_cm_pca)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(tree_cm_pca, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Decision Tree Confusion Matrix with PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

tree_class_report_pca = classification_report(y_test, y_pred_pca)
print("\nClassification Report:")
print(tree_class_report_pca)

report = classification_report(y_test, y_pred_pca, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Decision Tree Classification Report Heatmap with PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###Naive Bayes"""

gnb_model_pca = DecisionTreeClassifier()
gnb_model_pca.fit(X_train_pca, y_train)

y_pred_pca = gnb_model_pca.predict(X_test_pca)

"""Accuracy"""

gnb_accuracy_pca = accuracy_score(y_test, y_pred_pca)
print("Accuracy:", gnb_accuracy_pca)

"""Confusion Matrix"""

gnb_cm_pca = confusion_matrix(y_test, y_pred_pca)
print('confusion matrix:')
print(gnb_cm_pca)

"""Visualize Confusion Matrix"""

plt.figure(figsize=(6, 5))
sns.heatmap(gnb_cm_pca, annot=True, fmt='d', cmap='Blues', cbar=False)

plt.title('Naive Bayes Confusion Matrix with PCA')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks([0.5, 1.5], ['Not Depressed', 'Depressed'])
plt.yticks([0.5, 1.5], ['Not Depressed', 'Depressed'], rotation=0)

plt.show()

"""Classification Report"""

gnb_class_report_pca = classification_report(y_test, y_pred_pca)
print("\nClassification Report:")
print(gnb_class_report_pca)

report = classification_report(y_test, y_pred_pca, output_dict=True)
report_df = pd.DataFrame(report).transpose()

plt.figure(figsize=(8, 5))
sns.heatmap(report_df.iloc[:, :-1], annot=True, cmap='YlGnBu', fmt='.2f')  # exclude support column for better scaling

plt.title('Naive Bayes Classification Report Heatmap with PCA')
plt.ylabel('Classes')
plt.xlabel('Metrics')
plt.show()

"""###Bar Plot"""

accuracy_data = {
    'Model': ['Logistic Regression', 'XGBoost', 'Random Forest', 'SVM', 'KNN', 'Decision Tree', 'Naive Bayes'],
    'Accuracy': [logistic_accuracy_pca, xgb_accuracy_pca, rf_accuracy_pca, svm_accuracy_pca, knn_accuracy_pca, tree_accuracy_pca, gnb_accuracy_pca]
}

accuracy_df = pd.DataFrame(accuracy_data)

# Plotting the bar plot
plt.figure(figsize=(8, 6))
bars = plt.bar(accuracy_df['Model'], accuracy_df['Accuracy'], color='skyblue')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison with PCA')
plt.ylim(0, 1)  # Set y-axis limit to 0-1 for accuracy

# Rotate the x-axis labels to avoid overlap
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Adding accuracy labels inside the bars
for bar in bars:
    yval = bar.get_height()  # Get the height of the bar (accuracy value)
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.02, f'{yval:.3f}', ha='center', va='bottom', fontsize=10)

plt.show()